@article{Xiang2017,
abstract = {As in many neural network architectures, the use of Batch Normalization (BN) has become a common practice for Generative Adversarial Networks (GAN). In this paper, we propose using Euclidean reconstruction error on a test set for evaluating the quality of GANs. Under this measure, together with a careful visual analysis of generated samples, we found that while being able to speed training during early stages, BN may have negative effects on the quality of the trained model and the stability of the training process. Furthermore, Weight Normalization, a more recently proposed technique, is found to improve the reconstruction, training speed and especially the stability of GANs, and thus should be used in place of BN in GAN training.},
archivePrefix = {arXiv},
arxivId = {1704.03971},
author = {Xiang, Sitao and Li, Hao},
eprint = {1704.03971},
file = {:home/shishir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiang, Li - 2017 - On the effect of Batch Normalization and Weight Normalization in Generative Adversarial Networks.pdf:pdf;:home/shishir/Documents/pdfs/1704.03971.pdf:pdf},
pages = {1--29},
title = {{On the effect of Batch Normalization and Weight Normalization in Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1704.03971},
year = {2017}
}
@article{Gulrajani2017,
abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes significant progress toward stable training of GANs, but can still generate low-quality samples or fail to converge in some settings. We find that these training failures are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to pathological behavior. We propose an alternative method for enforcing the Lipschitz constraint: instead of clipping weights, penalize the norm of the gradient of the critic with respect to its input. Our proposed method converges faster and generates higher-quality samples than WGAN with weight clipping. Finally, our method enables very stable GAN training: for the first time, we can train a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.},
archivePrefix = {arXiv},
arxivId = {1704.00028},
author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
doi = {10.1016/j.aqpro.2013.07.003},
eprint = {1704.00028},
file = {:home/shishir/Documents/pdfs/1704.00028.pdf:pdf},
isbn = {0030-8870},
issn = {00308870},
pages = {1--19},
pmid = {24439530},
title = {{Improved Training of Wasserstein GANs}},
url = {http://arxiv.org/abs/1704.00028},
year = {2017}
}
@article{Shang2016,
abstract = {Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.},
archivePrefix = {arXiv},
arxivId = {1603.05201},
author = {Shang, Wenling and Sohn, Kihyuk and Almeida, Diogo and Lee, Honglak},
eprint = {1603.05201},
file = {:home/shishir/Documents/pdfs/1603.05201.pdf:pdf},
isbn = {9781510829008},
title = {{Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units}},
url = {http://arxiv.org/abs/1603.05201},
volume = {48},
year = {2016}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
doi = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
eprint = {1412.6980},
file = {:home/shishir/Documents/pdfs/1412.6980.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{c,
abstract = {Recent interest in distributed vector represen-tations for words has resulted in an increased diversity of approaches, each with strengths and weaknesses. We demonstrate how di-verse vector representations may be inexpen-sively composed into hybrid representations, effectively leveraging strengths of individual components, as evidenced by substantial im-provements on a standard word analogy task. We further compare these results over differ-ent sizes of training sets and find these ad-vantages are more pronounced when training data is limited. Finally, we explore the rela-tive impacts of the differences in the learning methods themselves and the size of the con-texts they access.},
author = {Garten, Justin and Sagae, Kenji and Ustun, Volkan and Dehghani, Morteza},
doi = {10.3115/v1/W15-1513},
file = {:home/shishir/Documents/pdfs/W15-1513.pdf:pdf},
journal = {Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing},
pages = {95--101},
title = {{Combining Distributed Vector Representations for Words}},
url = {http://aclweb.org/anthology/W15-1513},
year = {2015}
}
@misc{GoodfellowReddit2016,
author = {Goodfellow, Ian J.},
booktitle = {Reddit},
title = {{Generative Adversarial Networks for Text; Comment by Ian Goodfellow}},
url = {https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative{\_}adversarial{\_}networks{\_}for{\_}text/cyyp0nl/},
year = {2016}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1406.2661},
file = {:home/shishir/Documents/pdfs/1406.2661.pdf:pdf},
isbn = {1406.2661},
issn = {10495258},
pages = {1--9},
pmid = {15040217},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
file = {:home/shishir/Documents/pdfs/glove.pdf:pdf},
isbn = {9781937284961},
issn = {10495258},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
pages = {1532--1543},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
url = {http://aclweb.org/anthology/D14-1162},
year = {2014}
}
@article{Baldi2012,
abstract = {Autoencoders play a fundamental role in unsupervised learning and in deep architectures for transfer learning and other tasks. In spite of their fundamental role, only linear au- toencoders over the real numbers have been solved analytically. Here we present a general mathematical framework for the study of both linear and non-linear autoencoders. The framework allows one to derive an analytical treatment for the most non-linear autoen- coder, the Boolean autoencoder. Learning in the Boolean autoencoder is equivalent to a clustering problem that can be solved in polynomial time when the number of clusters is small and becomes NP complete when the number of clusters is large. The framework sheds light on the different kinds of autoencoders, their learning complexity, their horizontal and vertical composability in deep architectures, their critical points, and their fundamental connections},
author = {Baldi, Pierre},
file = {:home/shishir/Documents/pdfs/baldi12a.pdf:pdf},
issn = {0899-7667},
journal = {ICML Unsupervised and Transfer Learning},
keywords = {autoencoders,deep architectures,hebbian learning,information,unsupervised learning},
pages = {37--50},
title = {{Autoencoders, Unsupervised Learning, and Deep Architectures}},
year = {2012}
}
@article{Semeniuta2017,
abstract = {In this paper we explore the effect of architectural choices on learning a Variational Autoencoder (VAE) for text generation. In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends fully feed-forward convolutional and deconvolutional components with a recurrent language model. Our architecture exhibits several attractive properties such as faster run time and convergence, ability to better handle long sequences and, more importantly, it helps to avoid some of the major difficulties posed by training VAE models on textual data.},
archivePrefix = {arXiv},
arxivId = {1702.02390},
author = {Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
eprint = {1702.02390},
file = {:home/shishir/Documents/pdfs/1702.02390.pdf:pdf},
title = {{A Hybrid Convolutional Variational Autoencoder for Text Generation}},
url = {http://arxiv.org/abs/1702.02390},
year = {2017}
}
@article{Xiao2017,
abstract = {Generative models reduce the need of acquiring laborious labeling for the dataset. Text generation techniques can be applied for improving language models, machine translation, summarization, and captioning. This project experiments on different recurrent neural network models to build generative adversarial networks for generating texts from noise. The trained generator is capable of producing sentences with certain level of grammar and logic.},
author = {Xiao, X.},
file = {:home/shishir/Documents/pdfs/2761133.pdf:pdf},
title = {{Text Generation using Generative Adversarial Training}},
year = {2017}
}
@article{Vosoughi2016,
abstract = {We present Tweet2Vec, a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks. The evaluations demonstrate the power of the tweet embeddings generated by our model for various tweet categorization tasks. The vector representations generated by our model are generic, and hence can be applied to a variety of tasks. Though the model presented in this paper is trained on English-language tweets, the method presented can be used to learn tweet embeddings for different languages.},
archivePrefix = {arXiv},
arxivId = {1607.07514},
author = {Vosoughi, Soroush and Vijayaraghavan, Prashanth and Roy, Deb},
doi = {10.1145/2911451.2914762},
eprint = {1607.07514},
file = {:home/shishir/Documents/pdfs/tweet2vec{\_}vvr.pdf:pdf},
isbn = {9781450340694},
keywords = {cnn,convolutional neural networks,embedding,encoder-decoder,lstm,tweet,tweet2vec,twitter},
pages = {16--19},
title = {{Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM Encoder-Decoder}},
url = {http://arxiv.org/abs/1607.07514{\%}0Ahttp://dx.doi.org/10.1145/2911451.2914762},
year = {2016}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:home/shishir/Documents/pdfs/1511.06434.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Mescheder2017,
abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
archivePrefix = {arXiv},
arxivId = {1701.04722},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
eprint = {1701.04722},
file = {:home/shishir/Documents/pdfs/1701.04722.pdf:pdf},
title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.04722},
year = {2017}
}
@article{Makhzani2015,
abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
archivePrefix = {arXiv},
arxivId = {1511.05644},
author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
eprint = {1511.05644},
file = {:home/shishir/Documents/pdfs/1511.05644.pdf:pdf},
isbn = {9781509008063},
title = {{Adversarial Autoencoders}},
url = {http://arxiv.org/abs/1511.05644},
year = {2015}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:home/shishir/Documents/pdfs/1701.07875.pdf:pdf},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{DzmitryBahdana2014,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {{Dzmitry Bahdana} and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:home/shishir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dzmitry Bahdana et al. - 2014 - Neural Machine Translation By Jointly Learning To Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
journal = {Iclr 2015},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2014}
}
@article{Hamilton2016,
abstract = {Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.},
archivePrefix = {arXiv},
arxivId = {1605.09096},
author = {Hamilton, William L and Leskovec, Jure and Jurafsky, Dan},
eprint = {1605.09096},
file = {:home/shishir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamilton, Leskovec, Jurafsky - 2016 - Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.pdf:pdf},
isbn = {978-1-945626-00-5},
journal = {ACL 2016},
pages = {1489--1501},
title = {{Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change}},
url = {http://arxiv.org/abs/1605.09096},
year = {2016}
}
